{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capstone_ML_Rev4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOgMr+umWKinNhkbkbWaSjA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dreamtx01/Springboard/blob/master/Capstone_ML_Rev4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSGoP8aM8KZm",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2GW8HWJB9XW",
        "colab_type": "text"
      },
      "source": [
        "**1. IMPORT LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7K6eztsE81x",
        "colab_type": "code",
        "outputId": "b0cbb886-71a6-469d-be7a-7cc25197c0b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# This cell is to import the libraries\n",
        "\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import sklearn.model_selection\n",
        "from sklearn.model_selection import cross_val_score,train_test_split, GridSearchCV, KFold, cross_validate, cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import SelectKBest,RFECV\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.metrics import f1_score,confusion_matrix\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import r2_score, explained_variance_score, confusion_matrix, accuracy_score, classification_report, log_loss\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import VarianceThreshold,RFE,SelectFromModel,SelectKBest,f_classif, chi2, mutual_info_classif\n",
        "import warnings\n",
        "logreg=LogisticRegression()\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "print(\"Beginning\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_4OOIqKCizF",
        "colab_type": "text"
      },
      "source": [
        "**2. LOADING THE RAW DATA **\n",
        "**Regular Season Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W42kBgCRF8-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is to load the regular season detailed results and the Team spellings\n",
        "capstone = pd.read_csv(\"RegularSeasonDetailedResults.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9mAVSlGOyF5",
        "colab_type": "code",
        "outputId": "403e03f6-507b-418e-f598-b1878427b821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "capstone.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Season</th>\n",
              "      <th>DayNum</th>\n",
              "      <th>WTeamID</th>\n",
              "      <th>WScore</th>\n",
              "      <th>LTeamID</th>\n",
              "      <th>LScore</th>\n",
              "      <th>WLoc</th>\n",
              "      <th>NumOT</th>\n",
              "      <th>WFGM</th>\n",
              "      <th>WFGA</th>\n",
              "      <th>WFGM3</th>\n",
              "      <th>WFGA3</th>\n",
              "      <th>WFTM</th>\n",
              "      <th>WFTA</th>\n",
              "      <th>WOR</th>\n",
              "      <th>WDR</th>\n",
              "      <th>WAst</th>\n",
              "      <th>WTO</th>\n",
              "      <th>WStl</th>\n",
              "      <th>WBlk</th>\n",
              "      <th>WPF</th>\n",
              "      <th>LFGM</th>\n",
              "      <th>LFGA</th>\n",
              "      <th>LFGM3</th>\n",
              "      <th>LFGA3</th>\n",
              "      <th>LFTM</th>\n",
              "      <th>LFTA</th>\n",
              "      <th>LOR</th>\n",
              "      <th>LDR</th>\n",
              "      <th>LAst</th>\n",
              "      <th>LTO</th>\n",
              "      <th>LStl</th>\n",
              "      <th>LBlk</th>\n",
              "      <th>LPF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2003</td>\n",
              "      <td>10</td>\n",
              "      <td>1104</td>\n",
              "      <td>68</td>\n",
              "      <td>1328</td>\n",
              "      <td>62</td>\n",
              "      <td>N</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>58</td>\n",
              "      <td>3</td>\n",
              "      <td>14</td>\n",
              "      <td>11</td>\n",
              "      <td>18</td>\n",
              "      <td>14</td>\n",
              "      <td>24</td>\n",
              "      <td>13</td>\n",
              "      <td>23</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>53</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>16</td>\n",
              "      <td>22</td>\n",
              "      <td>10</td>\n",
              "      <td>22</td>\n",
              "      <td>8</td>\n",
              "      <td>18</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2003</td>\n",
              "      <td>10</td>\n",
              "      <td>1272</td>\n",
              "      <td>70</td>\n",
              "      <td>1393</td>\n",
              "      <td>63</td>\n",
              "      <td>N</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>62</td>\n",
              "      <td>8</td>\n",
              "      <td>20</td>\n",
              "      <td>10</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "      <td>28</td>\n",
              "      <td>16</td>\n",
              "      <td>13</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>24</td>\n",
              "      <td>67</td>\n",
              "      <td>6</td>\n",
              "      <td>24</td>\n",
              "      <td>9</td>\n",
              "      <td>20</td>\n",
              "      <td>20</td>\n",
              "      <td>25</td>\n",
              "      <td>7</td>\n",
              "      <td>12</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2003</td>\n",
              "      <td>11</td>\n",
              "      <td>1266</td>\n",
              "      <td>73</td>\n",
              "      <td>1437</td>\n",
              "      <td>61</td>\n",
              "      <td>N</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>58</td>\n",
              "      <td>8</td>\n",
              "      <td>18</td>\n",
              "      <td>17</td>\n",
              "      <td>29</td>\n",
              "      <td>17</td>\n",
              "      <td>26</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>25</td>\n",
              "      <td>22</td>\n",
              "      <td>73</td>\n",
              "      <td>3</td>\n",
              "      <td>26</td>\n",
              "      <td>14</td>\n",
              "      <td>23</td>\n",
              "      <td>31</td>\n",
              "      <td>22</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2003</td>\n",
              "      <td>11</td>\n",
              "      <td>1296</td>\n",
              "      <td>56</td>\n",
              "      <td>1457</td>\n",
              "      <td>50</td>\n",
              "      <td>N</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>38</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>17</td>\n",
              "      <td>31</td>\n",
              "      <td>6</td>\n",
              "      <td>19</td>\n",
              "      <td>11</td>\n",
              "      <td>12</td>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>18</td>\n",
              "      <td>18</td>\n",
              "      <td>49</td>\n",
              "      <td>6</td>\n",
              "      <td>22</td>\n",
              "      <td>8</td>\n",
              "      <td>15</td>\n",
              "      <td>17</td>\n",
              "      <td>20</td>\n",
              "      <td>9</td>\n",
              "      <td>19</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2003</td>\n",
              "      <td>11</td>\n",
              "      <td>1400</td>\n",
              "      <td>77</td>\n",
              "      <td>1208</td>\n",
              "      <td>71</td>\n",
              "      <td>N</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>61</td>\n",
              "      <td>6</td>\n",
              "      <td>14</td>\n",
              "      <td>11</td>\n",
              "      <td>13</td>\n",
              "      <td>17</td>\n",
              "      <td>22</td>\n",
              "      <td>12</td>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>24</td>\n",
              "      <td>62</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>17</td>\n",
              "      <td>27</td>\n",
              "      <td>21</td>\n",
              "      <td>15</td>\n",
              "      <td>12</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Season  DayNum  WTeamID  WScore  LTeamID  ...  LAst LTO  LStl  LBlk  LPF\n",
              "0    2003      10     1104      68     1328  ...     8  18     9     2   20\n",
              "1    2003      10     1272      70     1393  ...     7  12     8     6   16\n",
              "2    2003      11     1266      73     1437  ...     9  12     2     5   23\n",
              "3    2003      11     1296      56     1457  ...     9  19     4     3   23\n",
              "4    2003      11     1400      77     1208  ...    12  10     7     1   14\n",
              "\n",
              "[5 rows x 34 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75rBuK7qB5et",
        "colab_type": "text"
      },
      "source": [
        "**3. Data Preprocessing**\n",
        "\n",
        "**a. Dropping the neutral location so as to use home and away as target variable**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mYKa2O0wxaq",
        "colab_type": "code",
        "outputId": "84e268a6-db71-481a-e338-e4f779120f87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "capstone['WLoc'].value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "H    51825\n",
              "A    26759\n",
              "N     8920\n",
              "Name: WLoc, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yx94xDj_sQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "Neutral =capstone.loc[capstone.WLoc== \"N\",:]\n",
        "Neutral=capstone[capstone['WLoc']=='N'].index\n",
        "newcapstone=capstone.drop(Neutral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjY2FacAk864",
        "colab_type": "text"
      },
      "source": [
        "**b. Picking the features for the Logistic Regression, but dropping WLOC,Season,NumOT, WTID,LTID. Create a new dataframe called newcapstone for target variable (y) and newcapstonewoloc (X) for independent variable**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1MxqtwtTRel",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define y:  y= WLoc Set Home =1 and Way = 0\n",
        "newcapstone['WLoc2'] = [1 if val == 'H' else 0 for val in newcapstone['WLoc']]\n",
        "y=newcapstone.WLoc2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRdyiVYhTURt",
        "colab_type": "text"
      },
      "source": [
        "**Investigating the y variable**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5Wr-2aPe_mG",
        "colab_type": "code",
        "outputId": "3a096000-617c-40b7-e628-7507648e44ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "sns.countplot (x=\"WLoc2\",data =newcapstone,palette=\"Set3\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0319c7da58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g20VcjM19lA",
        "colab_type": "code",
        "outputId": "6edc4079-c480-4b5b-d31f-e62e38cea5a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "#Define X: Dataframe without WLOC\n",
        "newcapstonewowloc =newcapstone.drop(['WLoc',\"WLoc2\",\"DayNum\",\"Season\",\"WTeamID\",\"LTeamID\",\"NumOT\"], axis=1)\n",
        "X=newcapstonewowloc\n",
        "X.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>WScore</th>\n",
              "      <th>LScore</th>\n",
              "      <th>WFGM</th>\n",
              "      <th>WFGA</th>\n",
              "      <th>WFGM3</th>\n",
              "      <th>WFGA3</th>\n",
              "      <th>WFTM</th>\n",
              "      <th>WFTA</th>\n",
              "      <th>WOR</th>\n",
              "      <th>WDR</th>\n",
              "      <th>WAst</th>\n",
              "      <th>WTO</th>\n",
              "      <th>WStl</th>\n",
              "      <th>WBlk</th>\n",
              "      <th>WPF</th>\n",
              "      <th>LFGM</th>\n",
              "      <th>LFGA</th>\n",
              "      <th>LFGM3</th>\n",
              "      <th>LFGA3</th>\n",
              "      <th>LFTM</th>\n",
              "      <th>LFTA</th>\n",
              "      <th>LOR</th>\n",
              "      <th>LDR</th>\n",
              "      <th>LAst</th>\n",
              "      <th>LTO</th>\n",
              "      <th>LStl</th>\n",
              "      <th>LBlk</th>\n",
              "      <th>LPF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>81</td>\n",
              "      <td>55</td>\n",
              "      <td>26</td>\n",
              "      <td>57</td>\n",
              "      <td>6</td>\n",
              "      <td>12</td>\n",
              "      <td>23</td>\n",
              "      <td>27</td>\n",
              "      <td>12</td>\n",
              "      <td>24</td>\n",
              "      <td>12</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>18</td>\n",
              "      <td>20</td>\n",
              "      <td>46</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "      <td>12</td>\n",
              "      <td>17</td>\n",
              "      <td>6</td>\n",
              "      <td>22</td>\n",
              "      <td>8</td>\n",
              "      <td>19</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>80</td>\n",
              "      <td>62</td>\n",
              "      <td>23</td>\n",
              "      <td>55</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>32</td>\n",
              "      <td>39</td>\n",
              "      <td>13</td>\n",
              "      <td>18</td>\n",
              "      <td>14</td>\n",
              "      <td>17</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>19</td>\n",
              "      <td>41</td>\n",
              "      <td>4</td>\n",
              "      <td>15</td>\n",
              "      <td>20</td>\n",
              "      <td>28</td>\n",
              "      <td>9</td>\n",
              "      <td>21</td>\n",
              "      <td>11</td>\n",
              "      <td>30</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>84</td>\n",
              "      <td>56</td>\n",
              "      <td>32</td>\n",
              "      <td>67</td>\n",
              "      <td>5</td>\n",
              "      <td>17</td>\n",
              "      <td>15</td>\n",
              "      <td>19</td>\n",
              "      <td>14</td>\n",
              "      <td>22</td>\n",
              "      <td>11</td>\n",
              "      <td>6</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>23</td>\n",
              "      <td>52</td>\n",
              "      <td>3</td>\n",
              "      <td>14</td>\n",
              "      <td>7</td>\n",
              "      <td>12</td>\n",
              "      <td>9</td>\n",
              "      <td>23</td>\n",
              "      <td>10</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>106</td>\n",
              "      <td>50</td>\n",
              "      <td>41</td>\n",
              "      <td>69</td>\n",
              "      <td>15</td>\n",
              "      <td>25</td>\n",
              "      <td>9</td>\n",
              "      <td>13</td>\n",
              "      <td>15</td>\n",
              "      <td>29</td>\n",
              "      <td>21</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>17</td>\n",
              "      <td>52</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>12</td>\n",
              "      <td>17</td>\n",
              "      <td>8</td>\n",
              "      <td>15</td>\n",
              "      <td>8</td>\n",
              "      <td>17</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>76</td>\n",
              "      <td>48</td>\n",
              "      <td>25</td>\n",
              "      <td>56</td>\n",
              "      <td>10</td>\n",
              "      <td>23</td>\n",
              "      <td>16</td>\n",
              "      <td>23</td>\n",
              "      <td>8</td>\n",
              "      <td>35</td>\n",
              "      <td>18</td>\n",
              "      <td>13</td>\n",
              "      <td>14</td>\n",
              "      <td>19</td>\n",
              "      <td>13</td>\n",
              "      <td>18</td>\n",
              "      <td>64</td>\n",
              "      <td>8</td>\n",
              "      <td>24</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>14</td>\n",
              "      <td>26</td>\n",
              "      <td>12</td>\n",
              "      <td>17</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    WScore  LScore  WFGM  WFGA  WFGM3  WFGA3  ...  LDR  LAst  LTO  LStl  LBlk  LPF\n",
              "5       81      55    26    57      6     12  ...   22     8   19     4     3   25\n",
              "6       80      62    23    55      2      8  ...   21    11   30    10     4   28\n",
              "9       84      56    32    67      5     17  ...   23    10   18     1     3   18\n",
              "10     106      50    41    69     15     25  ...   15     8   17     7     3   15\n",
              "13      76      48    25    56     10     23  ...   26    12   17    10     0   17\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7C-HiLLKq9j",
        "colab_type": "text"
      },
      "source": [
        "**Reference for Feature Selection**\n",
        "https://scikit-learn.org/stable/modules/feature_selection.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI1T34L-YHQy",
        "colab_type": "text"
      },
      "source": [
        "**Feature Selection**\n",
        "\n",
        "1.   Split the data into X &y\n",
        "2.   Run a variance threshold\n",
        "3.   Run Chi square test and RFE\n",
        "3.   Rank features\n",
        "4.   Compare before and after feature selection for chi sq and RFE\n",
        "5.   Plot ROC and AUC curve\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oABuaM03afiz",
        "colab_type": "text"
      },
      "source": [
        "Before doing feature selection, we will need to split the data. The reason for this is that features are selected based on the information on the training set not on the whole data set. The test set is kept separate so as to evaluate the performance of the feature selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtotYIr7gU-T",
        "colab_type": "code",
        "outputId": "43854617-cd28-45f0-c5bc-701a022f4dcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(78584, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mvGKomrn6oP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.75, test_size=0.25, random_state=15,stratify=y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBofZXjyn6e8",
        "colab_type": "code",
        "outputId": "6a4160b0-5d47-47a2-cb08-b54ac749c55d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(58938, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwtZOyiDn6VD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# VarianceThreshold is a simple baseline approach to feature selection. It removes all features whose variance\n",
        "# doesnâ€™t meet some threshold. By default, it removes all zero-variance features, \n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2lEiThJn6LU",
        "colab_type": "code",
        "outputId": "66f5b45e-084f-4eee-85c4-edee000a47f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sel_variance_threshold = VarianceThreshold() \n",
        "X_train_remove_variance = sel_variance_threshold.fit_transform(X_train)\n",
        "X_train_remove_variance.shape\n",
        "# The data still has 28 features, no features were removed"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(58938, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IReObvHn5nJ",
        "colab_type": "code",
        "outputId": "d812abef-9dcc-4e1b-cb0f-67b1d540b894",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "source": [
        "# Chi square test. For more information about chi square test, read:\n",
        "# a.  sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2\n",
        "# source code on application of sklearn apply chi squre test: https://github.com/scikit-learn/scikit-learn/blob/1495f6924/sklearn/feature_selection/univariate_selection.py#L172\n",
        "\n",
        "#test = SelectKBest(score_func=chi2, k=5)\n",
        "\n",
        "\n",
        "#fit = test.fit(X, y)\n",
        "\n",
        "sel_chi2 = SelectKBest(score_func=chi2, k=10) # select top ten features\n",
        "X_train_chi2 = sel_chi2.fit_transform(X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "selected_chi2_features=pd.DataFrame({'Feature':list(X_train.columns),'Ranking':sel_chi2.scores_})\n",
        "selected_chi2_features.sort_values(by=\"Ranking\",ascending=False)\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Feature</th>\n",
              "      <th>Ranking</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>LFTA</td>\n",
              "      <td>2850.910886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>WAst</td>\n",
              "      <td>2539.789957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>LFTM</td>\n",
              "      <td>2093.096753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>WBlk</td>\n",
              "      <td>2090.789339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>WFTA</td>\n",
              "      <td>1814.632297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>LTO</td>\n",
              "      <td>1357.827103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>LBlk</td>\n",
              "      <td>1341.562519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>WPF</td>\n",
              "      <td>1268.878339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>WScore</td>\n",
              "      <td>1136.246609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>WFTM</td>\n",
              "      <td>1105.862560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>LPF</td>\n",
              "      <td>852.435963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>LAst</td>\n",
              "      <td>740.258827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LScore</td>\n",
              "      <td>530.205976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>WStl</td>\n",
              "      <td>505.492626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>WOR</td>\n",
              "      <td>438.926470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>WTO</td>\n",
              "      <td>280.439269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>WFGM</td>\n",
              "      <td>227.845125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>WFGA3</td>\n",
              "      <td>218.516502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>WFGA</td>\n",
              "      <td>212.973468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>LOR</td>\n",
              "      <td>106.583540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>LStl</td>\n",
              "      <td>28.307664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>LFGM3</td>\n",
              "      <td>25.713487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>WDR</td>\n",
              "      <td>21.072990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>LFGA3</td>\n",
              "      <td>15.860464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>LFGM</td>\n",
              "      <td>14.761191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>WFGM3</td>\n",
              "      <td>3.887772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>LFGA</td>\n",
              "      <td>1.689590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>LDR</td>\n",
              "      <td>1.086626</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Feature      Ranking\n",
              "20    LFTA  2850.910886\n",
              "10    WAst  2539.789957\n",
              "19    LFTM  2093.096753\n",
              "13    WBlk  2090.789339\n",
              "7     WFTA  1814.632297\n",
              "24     LTO  1357.827103\n",
              "26    LBlk  1341.562519\n",
              "14     WPF  1268.878339\n",
              "0   WScore  1136.246609\n",
              "6     WFTM  1105.862560\n",
              "27     LPF   852.435963\n",
              "23    LAst   740.258827\n",
              "1   LScore   530.205976\n",
              "12    WStl   505.492626\n",
              "8      WOR   438.926470\n",
              "11     WTO   280.439269\n",
              "2     WFGM   227.845125\n",
              "5    WFGA3   218.516502\n",
              "3     WFGA   212.973468\n",
              "21     LOR   106.583540\n",
              "25    LStl    28.307664\n",
              "17   LFGM3    25.713487\n",
              "9      WDR    21.072990\n",
              "18   LFGA3    15.860464\n",
              "15    LFGM    14.761191\n",
              "4    WFGM3     3.887772\n",
              "16    LFGA     1.689590\n",
              "22     LDR     1.086626"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-c3r5jtXD0w",
        "colab_type": "text"
      },
      "source": [
        "**3a.Leaving the test set aside and not using it in feature selection process**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouwDaqCFdcqL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), \n",
        "# recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. \n",
        "# First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through \n",
        "#a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current \n",
        "# set of features.That procedure is recursively repeated on the pruned set until the desired number of features to select \n",
        "# is eventually reached."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT9GmJ0add8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use the logistic regresssion as the model\n",
        "# about RFE in sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE\n",
        "\n",
        "model_logistic = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000)\n",
        "sel_rfe_logistic = RFE(estimator=model_logistic, n_features_to_select=10, step=1)\n",
        "X_train_rfe_logistic = sel_rfe_logistic.fit_transform(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2HVFa0rnCMr",
        "colab_type": "code",
        "outputId": "aca8e387-4243-486d-81d1-b22d7038cc9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train_rfe_logistic.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(58938, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJdF_Hlxddy2",
        "colab_type": "code",
        "outputId": "b1f383d2-1bdc-4825-f767-be6d47d7bf0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "source": [
        "# All the features with the value 1 are the most important features\n",
        "selected_rfe_features=pd.DataFrame({'Feature':list(X_train.columns),'Ranking':sel_rfe_logistic.ranking_})\n",
        "selected_rfe_features.sort_values(by=\"Ranking\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Feature</th>\n",
              "      <th>Ranking</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>WBlk</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>LTO</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>LAst</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>LOR</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>WPF</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>LBlk</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>WTO</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>WAst</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>LPF</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>WOR</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>LFTM</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>WStl</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>WFTM</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>LStl</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>LFGM3</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>WFGM3</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>WFGA3</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>WFGA</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>LDR</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LScore</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>WDR</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>LFTA</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>LFGA</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>LFGA3</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>WScore</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>WFGM</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>LFGM</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>WFTA</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Feature  Ranking\n",
              "13    WBlk        1\n",
              "24     LTO        1\n",
              "23    LAst        1\n",
              "21     LOR        1\n",
              "14     WPF        1\n",
              "26    LBlk        1\n",
              "11     WTO        1\n",
              "10    WAst        1\n",
              "27     LPF        1\n",
              "8      WOR        1\n",
              "19    LFTM        2\n",
              "12    WStl        3\n",
              "6     WFTM        4\n",
              "25    LStl        5\n",
              "17   LFGM3        6\n",
              "4    WFGM3        7\n",
              "5    WFGA3        8\n",
              "3     WFGA        9\n",
              "22     LDR       10\n",
              "1   LScore       11\n",
              "9      WDR       12\n",
              "20    LFTA       13\n",
              "16    LFGA       14\n",
              "18   LFGA3       15\n",
              "0   WScore       16\n",
              "2     WFGM       17\n",
              "15    LFGM       18\n",
              "7     WFTA       19"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdgzxw5glKuF",
        "colab_type": "text"
      },
      "source": [
        "**(4)Because of (3)-(b) above, I would recommend that you build\n",
        "two models (with the same train/test split from (1))**\n",
        "\n",
        "**(a) One with all the features **\n",
        "\n",
        "Compare the performance before and after the feature selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzSYktiNlP0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#(1) Before feature selection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5jcQefXddQe",
        "colab_type": "code",
        "outputId": "cd71966c-a8ec-4760-892d-978586520c1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "model_logistic = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=10000)\n",
        "model_logistic.fit(X_train, y_train)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
              "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7apraY0q4Jn",
        "colab_type": "text"
      },
      "source": [
        "**Evaluating the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvrrVGhhddO2",
        "colab_type": "code",
        "outputId": "a7dcdcf4-4f48-4cba-9b05-a01cc408a6aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "# use sklearn.metrics.classification_report for a more comprehensive\n",
        "# performance analysis\n",
        "# ref: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report\n",
        "predict = model_logistic.predict(X_test)\n",
        "print(confusion_matrix(y_test, predict))\n",
        "print(classification_report(y_test, predict))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 3009  3681]\n",
            " [ 1682 11274]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.45      0.53      6690\n",
            "           1       0.75      0.87      0.81     12956\n",
            "\n",
            "    accuracy                           0.73     19646\n",
            "   macro avg       0.70      0.66      0.67     19646\n",
            "weighted avg       0.72      0.73      0.71     19646\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7B7FtFwlh7E",
        "colab_type": "text"
      },
      "source": [
        "**(b) One with the features recommended by (3) chi sq**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcSs0SLdddDL",
        "colab_type": "code",
        "outputId": "c3524eda-8761-46cd-a40b-14f320f56a64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# we use the result from the feature selection based on the chi square test\n",
        "# X_train_chi2 is the data after the feature selection to feed into the model\n",
        "\n",
        "model_logistic = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=10000)\n",
        "model_logistic.fit(X_train_chi2, y_train)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
              "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2YI_-Dbdc4t",
        "colab_type": "code",
        "outputId": "6ec33e35-f122-41b7-aebf-9f74b5ebf840",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# We also need to transform the test data because the number of features were changed\n",
        "\n",
        "X_test_chi2 = sel_chi2.transform(X_test)\n",
        "print(X_test.shape)\n",
        "print(X_test_chi2.shape)\n",
        "\n",
        "# Only use the features in the test set that are corresponding to the remaining features in the training set. 4 features in this case"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19646, 28)\n",
            "(19646, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD4sfsqUdc0e",
        "colab_type": "code",
        "outputId": "68db06f2-9b1d-453a-db41-82db8529bae4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "predict = model_logistic.predict(X_test_chi2)\n",
        "print(confusion_matrix(y_test, predict))\n",
        "print(classification_report(y_test, predict))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 2310  4380]\n",
            " [ 1531 11425]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.35      0.44      6690\n",
            "           1       0.72      0.88      0.79     12956\n",
            "\n",
            "    accuracy                           0.70     19646\n",
            "   macro avg       0.66      0.61      0.62     19646\n",
            "weighted avg       0.68      0.70      0.67     19646\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZBltW4Rf96Z",
        "colab_type": "text"
      },
      "source": [
        "**(c) One with the features recommended by (3) RFE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO8528ScX7je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " #we use the result from the feature selection based on the RFE test\n",
        " #model_logistic is the data after the feature selection to feed into the model\n",
        " #model_logistic.fit(X_train_rfe_logistic, y_train)\n",
        "\n",
        "model_logistic = LogisticRegression(solver='lbfgs', max_iter=10000,random_state=10)\n",
        "sel_rfe_logistic=RFE(estimator=model_logistic,n_features_to_select=10,step=1)\n",
        "model_logistic = sel_rfe_logistic.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc3648zcX7RQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ea8dc71a-bfe9-48d1-b027-d30dc117a8ab"
      },
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "print(X_test.shape)\n",
        "print(X_train_rfe_logistic.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19646, 28)\n",
            "(58938, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE5R-jy1_9El",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a61ba9ff-d81c-417a-f64d-e3acc695cf89"
      },
      "source": [
        "predict.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19646,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhP1nFRUf5ff",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "25698a38-a6a3-492b-f43c-3797e4d06f7f"
      },
      "source": [
        "predict = model_logistic.predict(X_test)\n",
        "print(confusion_matrix(y_test, predict))\n",
        "print(classification_report(y_test, predict))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 2893  3797]\n",
            " [ 1683 11273]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.43      0.51      6690\n",
            "           1       0.75      0.87      0.80     12956\n",
            "\n",
            "    accuracy                           0.72     19646\n",
            "   macro avg       0.69      0.65      0.66     19646\n",
            "weighted avg       0.71      0.72      0.71     19646\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdztwVpspYmy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "58228786-15a0-4bed-f1f8-6ebdf960b20f"
      },
      "source": [
        "predict_train=model_logistic.predict(X_train)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, ..., 0, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRJLmXFXenWJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "acb4106e-4da1-4e57-ae3a-b6127ede1878"
      },
      "source": [
        "# save training accuracy for this split\n",
        "#tr_accuracy = accuracy_score(predict_train, y_train)\n",
        "     # save test accuracy for this split\n",
        "#tst_accuracy = accuracy_score(predict, y_test)\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7263225762665853"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BixsuEipkRpR",
        "colab_type": "code",
        "outputId": "e8d22f0e-1075-4258-8d19-40faf6868d38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# use sklearn.metrics.classification_report for a more comprehensive\n",
        "# performance analysis\n",
        "# ref: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report\n",
        "logreg.fit(X, y)\n",
        "#Accuracy on Train\n",
        "print(\"The Training Accuracy is: \", logreg.score(X_train, y_train))\n",
        "\n",
        "# Accuracy on Test\n",
        "print(\"The Testing Accuracy is: \", logreg.score(X_test, y_test))\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Training Accuracy is:  0.7286979537819404\n",
            "The Testing Accuracy is:  0.7274763310597577\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39i0kJU9mwWu",
        "colab_type": "text"
      },
      "source": [
        "**Final Model with Selected Features is the RFE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyhWyH3iuulU",
        "colab_type": "code",
        "outputId": "3950497b-b9ea-4ea4-ad81-f2c9e6be92b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# Training a Dummy Classifier\n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html?highlight=dummy#sklearn.dummy.DummyClassifier\n",
        "\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "dummy_clf = DummyClassifier(strategy=\"stratified\")\n",
        "dummy_clf.fit(X_train, y_train)\n",
        "score = dummy_clf.score(X_test, y_test)\n",
        "\n",
        "pred_proba_t = dummy_clf.predict_proba(X_test)\n",
        "\n",
        "\n",
        "print(\"Testing Acc:\", score)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Acc: 0.5497811259289422\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxlciIcxJGuY",
        "colab_type": "code",
        "outputId": "56ec4f01-a389-4ca7-f1ce-d458caa3fa07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Compute predicted probabilities: y_pred_prob\n",
        "\n",
        "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
        "logreg.fit(X, y)\n",
        "# Compute and print AUC score\n",
        "#print(\"AUC: {}\".format(roc_auc_score(y_predict_test, y_pred_prob)))\n",
        "print(\"AUC: {}\".format(roc_auc_score(predict, y_pred_prob)))\n",
        "# Compute cross-validated AUC scores: cv_auc\n",
        "cv_auc = cross_val_score(logreg,X,y,cv=5,scoring='roc_auc')\n",
        "\n",
        "# Print list of AUC scores\n",
        "print(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC: 0.9900571227047671\n",
            "AUC scores computed using 5-fold cross-validation: [0.78237612 0.76930074 0.76633225 0.75609879 0.74563077]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqDGHF7PI_Ek",
        "colab_type": "code",
        "outputId": "68e89cce-508a-4270-e749-12b738ac6496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "# Import necessary modules\n",
        "from sklearn.metrics import roc_curve\n",
        "logreg.fit(X, y)\n",
        "# Compute predicted probabilities: y_pred_prob\n",
        "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
        "\n",
        "# Generate ROC curve values: fpr, tpr, thresholds\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr, tpr)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'ROC Curve')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
